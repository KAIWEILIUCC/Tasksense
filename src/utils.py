import copy
import fcntl
import json
import os
import random
import base64
import json
import requests
import rsa

import cv2
import numpy as np
import rich
import torch
import yaml
from PIL import Image
from rich import align
from rich.panel import Panel
from torchvideotransforms import video_transforms, volume_transforms
from torchvision import transforms
from pydub import AudioSegment

from rich.console import Console
from rich.tree import Tree
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text
from rich.table import Table

from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich import box
from rich.text import Text


# read the configuration file
config_path = os.environ["CONFIG_PATH"]
config = yaml.load(open(config_path), Loader=yaml.FullLoader)
dataset_name = config["dataset_name"]
execution_cache_path = config["execution_settings"]["execution_cache_path"]
assessment_cache_path = config["execution_settings"]["assessment_cache_path"]

# Function to print a message
def print_tips(text_content, text_color="green", emoji=None, border=True):
    if emoji:
        text_content = f":{emoji}: {text_content}"
    
    if border:
        centered_content = align.Align(
            text_content,
            align="center",
            vertical="middle",
            style=f"bold {text_color}"
        )
        
        panel = Panel(
            centered_content,
            expand=True,
            border_style=text_color,
            width=80,
            style=f"bold {text_color}",
        )
        
        rich.print(panel)
    else:
        centered_content = align.Align(
            text_content,
            align="left",
            style=f"bold {text_color}"
        )
        rich.print(centered_content)

def visualize_plan(plan):
    console = Console()

    node_map = {}
    
    root = Tree("ğŸ¤– [bold magenta]Generated Plan[/bold magenta]", guide_style="bold cyan")
    
    for step in plan:
        step_id = step['id']
        tool_name = step['tool_name']
        dependencies = step['dep']
        args = step['args']
        
        title = Text.assemble(
            (f"Tool calling {step_id}", "bold yellow"),
            " : ",
            (f"{tool_name}", "bold green")
        )
        
        args_json = json.dumps(args, indent=2, ensure_ascii=False)
        if len(args_json.split('\n')) > 6:
            args_json = json.dumps(args, ensure_ascii=False)
            
        syntax = Syntax(args_json, "json", theme="monokai", word_wrap=True)
        
        content_table = Table.grid(padding=(0, 1))
        content_table.add_column()
        content_table.add_row(syntax)
        
        dep_str = ", ".join(map(str, dependencies))
        if dependencies == [-1]:
            dep_text = Text("Dependency: User Input (Start)", style="dim italic")
        else:
            dep_text = Text(f"Dependency: Tool Calling {dep_str}", style="dim")
        
        content_table.add_row(dep_text)

        step_panel = Panel(
            content_table,
            title=title,
            border_style="blue",
            expand=False
        )
        current_tree_node = None
        
        parent_found = False
        if dependencies == [-1]:
            current_tree_node = root.add(step_panel)
            parent_found = True
        else:
            first_dep = dependencies[0]
            if first_dep in node_map:
                current_tree_node = node_map[first_dep].add(step_panel)
                parent_found = True
                
        if not parent_found:
            current_tree_node = root.add(step_panel)

        node_map[step_id] = current_tree_node

    console.print()
    console.print(root)
    console.print()

def visualize_agent_response(text_content, title="ğŸ¤– Agent Inference", color="cyan", width=None):
    console = Console()
    
    md_content = Markdown(text_content)
    
    panel = Panel(
        md_content,
        title=f"[bold {color}]{title}[/]",
        title_align="left",      
        border_style=f"bold {color}",
        box=box.ROUNDED,         
        padding=(1, 2),          
        width=width,             
        subtitle="[dim italic]Generated by LLM Agent[/]", 
        subtitle_align="right"
    )
    console.print(panel)

# Function to pack the result
def pack_result(
    tool_calling_information,
    tool_calling_result,
):
    '''
    This function packs the result of the inference into a dictionary.

    Args:
        tool_calling_information (list): The plan list
        tool_calling_result (dict): The inference result of the plan

    Returns:
        dict: The packed result
    '''
    result = {
        "tool_calling_information": tool_calling_information,
        "tool_calling_result": tool_calling_result,
    }
    
    return result

# get a sign
def generate_a_sign():
    return hash(str(random.randint(0, 999999)))

# get video information
def get_video_info(video_path):
    '''
    This function gets the information of the video.

    Args:
        video_path (str): The path of the video

    Returns:
        tuple: a tuple containing the frame width, frame height, fps, and frame count
    '''
    
    cap = cv2.VideoCapture(video_path)
    
    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))
    fps = int(cap.get(5))   
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    return frame_width, frame_height, fps, frame_count

# return a normalized number of frames
class TemNormalizeLen(object):
    """Return a normalized number of frames."""

    def __init__(self, vid_len=(8, 32)):
        self.vid_len = vid_len

    def __call__(self, sample):
        num_frames = len(sample)
        indices = np.linspace(0, num_frames - 1, self.vid_len[0]).astype(int)
        _sample = []
        for i, f in enumerate(sample):
            if i in indices:
                _sample.append(f)

        return _sample

class MyVideoCapture:
    def __init__(self, source):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img

    def to_tensor(self, img):
        img = torch.from_numpy(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        return img.unsqueeze(0)

    def get_video_clip(self):
        assert len(self.stack) > 0, "clip length must large than 0 !"
        stack_copy = copy.deepcopy(self.stack)
        self.stack = [self.to_tensor(img) for img in self.stack]
        clip = torch.cat(self.stack).permute(-1, 0, 1, 2)
        del self.stack
        self.stack = []
        return clip, stack_copy

    def release(self):
        self.cap.release()

# video capture for object detection
class ObjectDetectionVideoCapture:
    def __init__(self, source):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img

    def to_tensor(self, img):
        img = torch.from_numpy(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        return img.unsqueeze(0)

    def get_video_clip(self):
        assert len(self.stack) > 0, "clip length must large than 0 !"
        stack_copy = copy.deepcopy(self.stack)
        self.stack = [self.to_tensor(img) for img in self.stack]
        clip = torch.cat(self.stack).permute(-1, 0, 1, 2)
        del self.stack
        self.stack = []
        return clip, stack_copy

    def release(self):
        self.cap.release()

# video capture for har
class HARVideoCapture:
    def __init__(self, source):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []
        self.imagenet_mean = [0.485, 0.456, 0.406]
        self.imagenet_std = [0.229, 0.224, 0.225]
        self.spa_transform = transforms.Compose(
            [
                video_transforms.Resize((224, 224)),
                volume_transforms.ClipToTensor(div_255=True),
                video_transforms.Normalize(
                    mean=self.imagenet_mean, std=self.imagenet_std
                ),
            ]
        )
        self.tem_transform = transforms.Compose([TemNormalizeLen((8, 8))])

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img

    def to_img(self, img):
        img = Image.fromarray(img.astype(np.uint8))
        return img

    def get_video_clip(self):
        assert len(self.stack) > 0, "clip length must large than 0 !"
        stack_copy = copy.deepcopy(self.stack)
        self.stack = self.tem_transform(self.stack)
        self.stack = [self.to_img(img) for img in self.stack]
        clip = self.spa_transform(self.stack)
        del self.stack
        self.stack = []
        return clip, stack_copy

    def release(self):
        self.cap.release()

class FaceRcognitionVideoCapture:
    def __init__(self, source):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((160, 160), antialias=True),
        ])

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img

    def to_tensor(self, img):
        img = self.transform(img)
        return img.unsqueeze(0)

    def get_video_clip(self):
        assert len(self.stack) > 0, "clip length must large than 0 !"
        stack_copy = copy.deepcopy(self.stack)
        self.stack = [self.to_tensor(img) for img in self.stack]
        clip = torch.cat(self.stack)
        del self.stack
        self.stack = []
        return clip, stack_copy

    def release(self):
        self.cap.release()
        
class VideoClassificationVideoCapture:
    def __init__(self, source, preprocess):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []
        self.preprocess = preprocess

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img

    def to_tensor(self, img):
        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        img = self.preprocess(img)
        return img.unsqueeze(0)

    def get_video_clip(self):
        assert len(self.stack) > 0, "clip length must be larger than 0!"
        stack_copy = copy.deepcopy(self.stack)
        self.stack = [self.to_tensor(img) for img in self.stack]
        clip_images = torch.cat(self.stack)
        del self.stack
        self.stack = []
        return clip_images, stack_copy

    def release(self):
        self.cap.release()

class GenderRecognitionVideoCapture:
    def __init__(self, source, pipe, batch_size=32):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []
        self.pipe = pipe
        self.batch_size = batch_size

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img

    def get_predictions(self):
        assert len(self.stack) > 0, "Batch size must be larger than 0!"
        images = [Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) for img in self.stack]
        # Use the pipeline to make predictions on the batch of images
        predictions = self.pipe(images, batch_size=self.batch_size)
        # Clear the stack for the next batch
        self.stack = []
        return predictions

    def release(self):
        self.cap.release()
        
class AgeRecognitionVideoCapture:
    def __init__(self, source, pipe, batch_size=32):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []
        self.pipe = pipe
        self.batch_size = batch_size
        
    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img
    
    def get_predictions(self):
        assert len(self.stack) > 0, "Batch size must be larger than 0!"
        images = [Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) for img in self.stack]
        # Use the pipeline to make predictions on the batch of images
        predictions = self.pipe(images, batch_size=self.batch_size)
        # Clear the stack for the next batch
        self.stack = []
        return predictions
    
    def release(self):
        self.cap.release()

class FacialExpressionRecognitionVideoCapture:
    def __init__(self, source, pipe, batch_size=32):
        self.cap = cv2.VideoCapture(source)
        self.idx = -1
        self.end = False
        self.stack = []
        self.pipe = pipe
        self.batch_size = batch_size
        
    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if ret:
            self.stack.append(img)
        else:
            self.end = True
        return ret, img
    
    def get_predictions(self):
        assert len(self.stack) > 0, "Batch size must be larger than 0!"
        images = [Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) for img in self.stack]
        # Use the pipeline to make predictions on the batch of images
        predictions = self.pipe(images, batch_size=self.batch_size)
        # Clear the stack for the next batch
        self.stack = []
        return predictions
    
    def release(self):
        self.cap.release()

# Function to convert the detections to the required format
def convert_detection_object_detection(detections, threshold, classes):
    '''
    This function converts the detections to the required format.

    Args:
        detections (dict): The detection results
        threshold (float): The threshold value for the confidence score
        classes (list): The list of classes to be considered

    Returns:
        list: The converted detections
    '''
    # Get the bounding boxes, labels and scores from the detections dictionary.
    boxes = detections["boxes"].cpu().numpy()
    labels = detections["labels"].cpu().numpy()
    scores = detections["scores"].cpu().numpy()
    # print(labels)
    # print(classes)
    lbl_mask = np.isin(labels, classes)
    # print(lbl_mask)
    scores = scores[lbl_mask]
    # print(scores)
    # Filter out low confidence scores and non-person classes.
    mask = scores > threshold
    boxes = boxes[lbl_mask][mask]
    scores = scores[mask]
    labels = labels[lbl_mask][mask]
    # print(boxes)
    # Convert boxes to [x1, y1, w, h, score] format.
    final_boxes = []
    for i, box in enumerate(boxes):
        # Append ([x, y, w, h], score, label_string).
        final_boxes.append(
            (
                [box[0], box[1], box[2] - box[0], box[3] - box[1]],
                scores[i],
                str(int(labels[i])),
            )
        )

    return final_boxes

# Load the cache with lock
def load_cache_with_lock(cache_path):
    '''
    This function loads the cache with lock.
    It reads the cache data from the cache file and returns it in the form of a dictionary.

    Args:
        cache_path (str): The path to the cache file

    Returns:
        dict: The cache data
    '''
    
    with open(cache_path, "r") as file:
        # aquiring the file descriptor
        fd = file.fileno()

        # attempts to acquire an exclusive lock
        fcntl.flock(fd, fcntl.LOCK_EX)

        try:
            # load the cache data
            cache = json.load(file)
        finally:
            # release the lock
            fcntl.flock(fd, fcntl.LOCK_UN)

    return cache

# Save the cache with lock
def save_cache_with_lock(cache_path, cache, tool_name, data_path):
    '''
    This function saves the cache with lock.

    Args:
        cache_path (str): The path to the cache file
        cache (dict): The cache data to be saved
        task_name (str): The task name
        data_path (str): The path of the raw data processed by the tool
    '''
    
    with open(cache_path, "r+") as file:
        # aquiring the file descriptor
        fd = file.fileno()

        # attempts to acquire an exclusive lock
        fcntl.flock(fd, fcntl.LOCK_EX)

        try:
            # Read the current cache data
            current_cache = json.load(file)

            # update the corresponding cache data of the current process
            current_cache[tool_name][data_path] = cache[tool_name][data_path]

            # Move the file pointer to the beginning of the file
            file.seek(0)

            # Write the updated cache data
            json.dump(current_cache, file, indent=4)

            # Truncate the file to remove the old data
            file.truncate()

            # Ensure that the data is written to the disk
            file.flush()
            os.fsync(fd)
        finally:
            # release the lock
            fcntl.flock(fd, fcntl.LOCK_UN)
            
def save_flops_cache_with_lock(cache_path, cache, flops_key):
    '''
    This function saves the cache with lock.

    Args:
        cache_path (str): The path to the cache file
        cache (dict): The cache data to be saved
        flops_key (str): The key of the flops data

    Returns:
        None
    '''
    
    with open(cache_path, "r+") as file:
        # aquiring the file descriptor
        fd = file.fileno()

        # attempts to acquire an exclusive lock
        fcntl.flock(fd, fcntl.LOCK_EX)

        try:
            # Read the current cache data
            current_cache = json.load(file)
            
            # update the corresponding cache data of the current process
            current_cache[flops_key] = cache[flops_key]
            
            # Move the file pointer to the beginning of the file
            file.seek(0)
            
            # Write the updated cache data
            json.dump(current_cache, file, indent=4)
            
            # Truncate the file to remove the old data
            file.truncate()
            
            # Ensure that the data is written to the disk
            file.flush()
            
            os.fsync(fd)
        finally:
            # release the lock
            fcntl.flock(fd, fcntl.LOCK_UN)

def save_embedding_cache_with_lock(cache_path, cache, sentense):
    with open(cache_path, "r+") as file:
        # è·å–æ–‡ä»¶æè¿°ç¬¦
        fd = file.fileno()

        # å°è¯•è·å–æ’å®ƒé”
        fcntl.flock(fd, fcntl.LOCK_EX)

        try:
            # è¯»å–å½“å‰çš„ç¼“å­˜æ•°æ®
            current_cache = json.load(file)

            # æ›´æ–°å½“å‰è¿›ç¨‹å¯¹åº”çš„ç¼“å­˜æ•°æ®
            current_cache[sentense] = cache[sentense]

            # å°†æ–‡ä»¶æŒ‡é’ˆç§»åŠ¨åˆ°æ–‡ä»¶å¼€å¤´
            file.seek(0)

            # å†™å…¥ç¼“å­˜æ•°æ®
            json.dump(current_cache, file, indent=4)

            # æˆªæ–­æ–‡ä»¶,åˆ é™¤å¤šä½™çš„æ—§æ•°æ®
            file.truncate()

            # ç¡®ä¿æ•°æ®å·²ç»å†™å…¥ç£ç›˜
            file.flush()
            os.fsync(fd)
        finally:
            # é‡Šæ”¾æ–‡ä»¶é”
            fcntl.flock(fd, fcntl.LOCK_UN)

# Empty the output folder
def empty_output_folder():
    '''
    This function empty the output folder.
    '''
    intermediate_output_path = config["execution_settings"]["intermediate_output_path"][dataset_name]
    
    try:
        for folder in os.listdir(intermediate_output_path):
            folder_path = os.path.join(intermediate_output_path, folder)
            for file in os.listdir(folder_path):
                file_path = os.path.join(folder_path, file)
                os.remove(file_path)
    except Exception as e:
        print(e)
        
    print_tips("Output folder emptied.", text_color="yellow", emoji="white_check_mark", border=False)


# Function to print the plan
def replace_slot(text, entries):
    """
    This function replaces the slot in the text.
    
    Args:
        text (str): The text with slots
        entries (dict): The entries to replace the slots
    """
    for key, value in entries.items():
        if not isinstance(value, str):
            value = str(value)

        text = text.replace(
            "{{" + key + "}}", value.replace('"', "'").replace("\n", "")
        )
    return text

# Initialize the cache with lock
def init_execution_cache():
    from src.planning.vocabulary_set_manager import \
        get_all_vocabulary_set_for_tool_type
    
    model_task_list = get_all_vocabulary_set_for_tool_type("model_tool")
    
    cache_path = execution_cache_path
    cache = {}
    
    for tool in model_task_list:
        if tool["tool_name"] not in cache.keys():
            cache[tool["tool_name"]] = {}
    
    with open(cache_path, "w") as file:
        fd = file.fileno()
        fcntl.flock(fd, fcntl.LOCK_EX)
        
        try:
            file.seek(0)
            json.dump(cache, file, indent=4)
            file.truncate()
        
            file.flush()
            os.fsync(fd)
        finally:
            fcntl.flock(fd, fcntl.LOCK_UN)
            
    print_tips("Execution cache initialized.", text_color="yellow", emoji="white_check_mark", border=False)
    
def init_assessment_cache():
    cache_path = assessment_cache_path
    cache = {}
    
    cache["rgb-video-quality-assessment"] = {}
    cache["depth-video-quality-assessment"] = {}
    
    with open(cache_path, "w") as file:
        fd = file.fileno()
        fcntl.flock(fd, fcntl.LOCK_EX)
        
        try:
            file.seek(0)
            json.dump(cache, file, indent=4)
            file.truncate()
        
            file.flush()
            os.fsync(fd)
        finally:
            fcntl.flock(fd, fcntl.LOCK_UN)
    
    print_tips("Assessment cache initialized.", text_color="yellow", emoji="white_check_mark", border=False)        
    
# visualize the .graphml file
def visualize_graphml(graphml_file, tool_name_id_mapping_path, output_path):
    import matplotlib.pyplot as plt
    import networkx as nx
    
    tool_name_id_mapping = json.load(open(tool_name_id_mapping_path))
    tool_id_name_mapping = {str(v): k for k, v in tool_name_id_mapping.items()}
    
    try:
        # read the graphml file
        G = nx.read_graphml(graphml_file)
        
        node_labels = {}
        for node_id in G.nodes():
            if node_id in tool_id_name_mapping:
                node_labels[node_id] = tool_id_name_mapping[node_id]
        
        # create a new figure
        fig, ax = plt.subplots(figsize=(60, 60))
        
        # draw the graph
        # pos = nx.fruchterman_reingold_layout(G)
        # pos = nx.spring_layout(G)
        # pos = nx.circular_layout(G)
        # pos = nx.spectral_layout(G)
        # pos = nx.shell_layout(G)
        # pos = nx.kamada_kawai_layout(G)
        pos = nx.random_layout(G)

        nx.draw(G, pos, with_labels=True, node_shape='o', node_size=1000, node_color='skyblue', font_size=20, font_weight='bold', edge_color='black', width=4, ax=ax)
        
        # remove the axis
        ax.axis('off')
        
        plt.subplots_adjust(left=0.4, right=0.6, top=0.6, bottom=0.4)
        
        # save the figure
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        
        print(f"Visualization of the graphml file has been saved to {output_path}")
    
    except FileNotFoundError:
        print(f"FilenotFoundError: {graphml_file} not found")
    
    except nx.NetworkXError as e:
        print(f"NetworkXError: {str(e)}")
    
    except Exception as e:
        print(f"Error: {str(e)}")
    
    finally:
        # close the figure
        plt.close()

def convert_to_wav(file_path):
    """
    Convert an audio file to WAV format if it's not already.
    
    args:
        file_path (str): The path to the audio file
        
    returns:
        str: The path to the converted
    """
    extension = os.path.splitext(file_path)[1]

    if extension != ".wav":
        audio = AudioSegment.from_file(file_path)

        new_file_path = os.path.splitext(file_path)[0] + ".wav"
        audio.export(new_file_path, format="wav")
        os.remove(file_path)
        # print(f"File converted to {new_file_path}")

        return new_file_path
    else:
        # print("File is already in WAV format.")

        return file_path

def merge_intervals(speaker_list, time_segment_list):
    '''
    This function merges the time segments of the same speaker.
    Impletation details:
    1. Initialize the result list.
    2. Initialize the current speaker and time segment.
    3. Iterate through the speaker list and time segment list.
    4. If the current speaker is the same as the previous one, extend the current time segment.
    5. If the speaker changes, add the previous time segment and speaker to the result list.
    6. Add the last time segment to the result list.

    Args:
        speaker_list (list): The list of speakers
        time_segment_list (list): The list of time segments

    Raises:
        ValueError: If the length of the speaker list and time segment list are not the same

    Returns:
        list: The merged time segments
    '''
    
    if len(speaker_list) != len(time_segment_list):
        raise ValueError("Speaker list and time segment list must have the same length.")
    
    # initialize the result list
    merged_segments = []
    
    # initialize the current speaker and time segment
    current_speaker = speaker_list[0]
    current_segment = list(time_segment_list[0])
    
    # iterate through the speaker list and time segment list
    for i in range(1, len(speaker_list)):
        # if the current speaker is the same as the previous one, extend the current time segment
        if speaker_list[i] == current_speaker:
            # extend the current time segment
            current_segment[1] = time_segment_list[i][1]
        else:
            # add the previous time segment and speaker to the result list
            merged_segments.append(current_segment)
            current_speaker = speaker_list[i]
            current_segment = list(time_segment_list[i])
    
    # add the last time segment to the result list
    merged_segments.append(current_segment)
    
    return merged_segments

def cut_and_save_audio(audio_path, start_time, end_time, output_path):
    '''
    This function cuts a segment from the audio at the given path and save it to the given output path.

    Args:
        audio_path (str): The path of the audio
        start_time (int): The start time of the segment
        end_time (int): The end time of the segment
        output_path (str): The path to save the audio segment
    '''
    
    audio = AudioSegment.from_wav(audio_path)
    audio_segment = audio[start_time * 1000:end_time * 1000]
    audio_segment.export(output_path, format="wav")

def split_audio(start_and_end_list, speaker_list, original_audio_idx, original_audio_path, output_dir):
    '''
    Split the audio at the given path into segments based on the given data,
    and save the segments to the given output directory.

    Args:
        start_and_end_list (list): The list of start and end times for each segment
        speaker_list (list): The list of speakers for each segment
        original_audio_idx (int): The index of the original audio
        original_audio_path (str): The path of the original audio
        output_dir (str): The directory to save the audio segments
        
    Returns:
        list: The list of paths of the audio segments
    '''
    
    split_file_path_list = []
    
    for i in range(len(start_and_end_list)):
        start_time = start_and_end_list[i][0]
        end_time = start_and_end_list[i][1]
        
        # if the interval is too short, skip it
        if end_time - start_time < 1:
            continue
        
        speaker = speaker_list[i]
        
        # generate the output file name
        output_path = f"{output_dir}/audio_{original_audio_idx}_speaker_{speaker}_start_{start_time}_end_{end_time}.wav"
        cut_and_save_audio(
            original_audio_path,
            start_time,
            end_time,
            output_path
        )
        
        split_file_path_list.append(output_path)
    
    return split_file_path_list

def convert_speech_file_to_array(path, sr=16000):
    '''
    This function converts the speech file at the given path to an array.

    Args:
        path (str): The path of the speech file
        sr (int, optional): _description_. Defaults to 16000.

    Returns:
        np.array: The array of the speech
    '''
    if isinstance(path, str):
        sound = AudioSegment.from_file(path)
    else:
        sound = path
    sound = sound.set_frame_rate(sr)
    sound_array = np.array(sound.get_array_of_samples()).astype(np.float32)
    return sound_array    
    
# tplink camera utils

def tp_encrypt(password):
    a = 'RDpbLfCPsJZ7fiv'
    c = 'yLwVl0zKqws7LgKPRQ84Mdt708T1qQ3Ha7xv3H7NyU84p21BriUWBU43odz3iP4rBL3cD02KZciXTysVXiV8ngg6vL48rPJyAUw0HurW20xqxv9aYb4M9wK1Ae0wlro510qXeU07kV57fQMc8L6aLgMLwygtc0F10a0Dg70TOoouyFhdysuRMO51yY5ZlOZZLEal1h0t9YQW0Ko7oBwmCAHoic4HYbUyVeU3sfQ1xtXcPcf1aT303wAQhv66qzW '
    b = password
    e = ''
    f, g, h, k, l = 187, 187, 187, 187, 187
    n = 187
    g = len(a)
    h = len(b)
    k = len(c)
    if g > h:
        f = g
    else:
        f = h
    for p in list(range(0, f)):
        n = l = 187
        if p >= g:
            n = ord(b[p])
        else:
            if p >= h:
                l = ord(a[p])
            else:
                l = ord(a[p])
                n = ord(b[p])
        e += c[(l ^ n) % k]
    return e

def convert_rsa_key(s):
    b_str = base64.b64decode(s)
    if len(b_str) < 162:
        return False
    hex_str = b_str.hex()
    m_start = 29 * 2
    e_start = 159 * 2
    m_len = 128 * 2
    e_len = 3 * 2
    modulus = hex_str[m_start:m_start + m_len]
    exponent = hex_str[e_start:e_start + e_len]
    return modulus, exponent

def rsa_encrypt(string, pubkey):
    key = convert_rsa_key(pubkey)
    modulus = int(key[0], 16)
    exponent = int(key[1], 16)
    rsa_pubkey = rsa.PublicKey(modulus, exponent)
    crypto = rsa.encrypt(string.encode(), rsa_pubkey)
    return base64.b64encode(crypto)

def get_stok(url, username, password):
    # encrypt tp
    tp_password = tp_encrypt(password)
    # login
    d = {
        "method": "do",
        "login": {
            "username": username,
            "password": tp_password
        }
    }
    print("--login")
    j = post_data(url, json.dumps(d))
    stok = j["stok"]
    return stok

def post_data(base_url, data, stok=""):
    url = base_url + (("/stok=" + stok + "/ds") if stok else "")
    r = requests.post(url, data)
    return r.json()

def escape_quotes_after_possible_labels(s):
    result = ''
    i = 0
    length = len(s)
    while i < length:
        # æ£€æŸ¥æ˜¯å¦é‡åˆ° 'Possible labels:'
        if s.startswith('Possible labels:', i) or s.startswith('label collection:', i):
            # æ·»åŠ  'Possible labels:' åˆ°ç»“æœä¸­
            if s.startswith('Possible labels:', i):
                result += 'Possible labels:'
                i += len('Possible labels:')
            elif s.startswith('label collection:', i):
                result += 'label collection:'
                i += len('label collection:')
            # å¤„ç†æ¥ä¸‹æ¥çš„å†…å®¹ï¼ŒæŸ¥æ‰¾æ–¹æ‹¬å·å†…çš„å†…å®¹
            while i < length and s[i] != '[':
                result += s[i]
                i +=1
            if i < length and s[i] == '[':
                # è¿›å…¥æ–¹æ‹¬å·å†…éƒ¨
                result += s[i]  # æ·»åŠ  '['
                i +=1
                in_brackets = True
                while i < length and in_brackets:
                    if s[i] == '"' and s[i-1] != '\\':
                        # æ–¹æ‹¬å·å†…çš„åŒå¼•å·ï¼Œæ›¿æ¢ä¸º '\\"'
                        result += '\\"'
                        i +=1
                    elif s[i] == ']':
                        # ç»“æŸæ–¹æ‹¬å·
                        result += s[i]
                        i +=1
                        in_brackets = False
                    else:
                        result += s[i]
                        i +=1
        else:
            # å…¶ä»–å­—ç¬¦ï¼Œç›´æ¥æ·»åŠ åˆ°ç»“æœä¸­
            result += s[i]
            i +=1
    return result